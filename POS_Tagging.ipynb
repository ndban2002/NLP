{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlkhrnVsQxcB"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('treebank')\n",
        "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
        "\n",
        "print(\"Tagged sentences: \", len(tagged_sentences))\n"
      ],
      "metadata": {
        "id": "_CeASeqUQ3zz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7102fba7-b410-4e47-e3a0-af376c42dfd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tagged sentences:  3914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hidden Markov Model"
      ],
      "metadata": {
        "id": "qC2l__yAbDng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import hmm\n",
        "\n",
        "\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "tagger = trainer.train_supervised(tagged_sentences)\n",
        "\n",
        "print(tagger)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pHKgdouRYBm",
        "outputId": "17268eb3-76ab-4aaa-f264-b783d2e5eb12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<HiddenMarkovModelTagger 46 states and 12408 output symbols>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "y_test= []\n",
        "split_index = round(len(tagged_sentences) * 0.8)\n",
        "\n",
        "for i in tagged_sentences[split_index:]:\n",
        "  X = [word for word, _ in i]\n",
        "  y_test.append([tag for _, tag in i])\n",
        "  predictions.append([tag for _, tag in tagger.tag(X)])"
      ],
      "metadata": {
        "id": "VEK-ZBC_bt3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_of_model(y_test, y_pred):\n",
        "  accuracy = 0\n",
        "  for i in range(len(y_test)):\n",
        "    if y_test[i] == y_pred[i]:\n",
        "      accuracy+=1\n",
        "  return accuracy/len(y_test)\n",
        "print(accuracy_of_model(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3pfMJjUZILi",
        "outputId": "f384ee74-aacf-4f49-f54e-69b4c0eb6678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6819923371647509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "input_sent = str(input())\n",
        "tokens = word_tokenize(input_sent)\n",
        "tags = tagger.tag(tokens)\n",
        "print(tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1jClLQ9RytA",
        "outputId": "5640157b-5c03-4e97-d1c7-62d22098f547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am Peter\n",
            "[('I', 'PRP'), ('am', 'VBP'), ('Peter', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CRF"
      ],
      "metadata": {
        "id": "Ny8lCTHRAz_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def features(sentence, index):\n",
        "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
        "    return {\n",
        "        'word': sentence[index],\n",
        "        'is_first': index == 0,\n",
        "        'is_last': index == len(sentence) - 1,\n",
        "        'is_capitalized': str(sentence[index][0]).upper() == sentence[index][0],\n",
        "        'is_all_caps': str(sentence[index]).upper() == sentence[index],\n",
        "        'is_all_lower': str(sentence[index]).lower() == sentence[index],\n",
        "        'prefix-1': sentence[index][0],\n",
        "        'prefix-2': sentence[index][:2],\n",
        "        'prefix-3': sentence[index][:3],\n",
        "        'suffix-1': sentence[index][-1],\n",
        "        'suffix-2': sentence[index][-2:],\n",
        "        'suffix-3': sentence[index][-3:],\n",
        "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
        "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
        "        'has_hyphen': '-' in sentence[index],\n",
        "        'is_numeric': str(sentence[index]).isdigit(),\n",
        "        'capitals_inside': str(sentence[index][1:]).lower() != sentence[index][1:]\n",
        "    }"
      ],
      "metadata": {
        "id": "RNlxAkFu_gxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag.util import untag\n",
        "\n",
        "# Split the dataset for training and testing\n",
        "cutoff = int(.75 * len(tagged_sentences))\n",
        "training_sentences = tagged_sentences[:cutoff]\n",
        "test_sentences = tagged_sentences[cutoff:]\n",
        "\n",
        "def transform_to_dataset(tagged_sentences):\n",
        "    X, y = [], []\n",
        "\n",
        "    for tagged in tagged_sentences:\n",
        "        X.append([features(untag(tagged), index) for index in range(len(tagged))])\n",
        "        y.append([tag for _, tag in tagged])\n",
        "\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = transform_to_dataset(training_sentences)\n",
        "X_test, y_test = transform_to_dataset(test_sentences)\n",
        "\n",
        "print(len(X_train))\n",
        "print(len(X_test))\n",
        "print(X_test[0])\n",
        "print(y_test[0])\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy7rDzxq9mK0",
        "outputId": "be599684-c3a9-412a-dc7e-2389a2d7edee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2935\n",
            "979\n",
            "[{'word': 'We', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'prefix-1': 'W', 'prefix-2': 'We', 'prefix-3': 'We', 'suffix-1': 'e', 'suffix-2': 'We', 'suffix-3': 'We', 'prev_word': '', 'next_word': 'can', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'can', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'c', 'prefix-2': 'ca', 'prefix-3': 'can', 'suffix-1': 'n', 'suffix-2': 'an', 'suffix-3': 'can', 'prev_word': 'We', 'next_word': 'understand', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'understand', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'u', 'prefix-2': 'un', 'prefix-3': 'und', 'suffix-1': 'd', 'suffix-2': 'nd', 'suffix-3': 'and', 'prev_word': 'can', 'next_word': 'and', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'and', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'a', 'prefix-2': 'an', 'prefix-3': 'and', 'suffix-1': 'd', 'suffix-2': 'nd', 'suffix-3': 'and', 'prev_word': 'understand', 'next_word': 'share', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'share', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 's', 'prefix-2': 'sh', 'prefix-3': 'sha', 'suffix-1': 'e', 'suffix-2': 're', 'suffix-3': 'are', 'prev_word': 'and', 'next_word': 'the', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'the', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 't', 'prefix-2': 'th', 'prefix-3': 'the', 'suffix-1': 'e', 'suffix-2': 'he', 'suffix-3': 'the', 'prev_word': 'share', 'next_word': 'compassion', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'compassion', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'c', 'prefix-2': 'co', 'prefix-3': 'com', 'suffix-1': 'n', 'suffix-2': 'on', 'suffix-3': 'ion', 'prev_word': 'the', 'next_word': 'that', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'that', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 't', 'prefix-2': 'th', 'prefix-3': 'tha', 'suffix-1': 't', 'suffix-2': 'at', 'suffix-3': 'hat', 'prev_word': 'compassion', 'next_word': '*T*-2', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': '*T*-2', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': '*', 'prefix-2': '*T', 'prefix-3': '*T*', 'suffix-1': '2', 'suffix-2': '-2', 'suffix-3': '*-2', 'prev_word': 'that', 'next_word': 'makes', 'has_hyphen': True, 'is_numeric': False, 'capitals_inside': True}, {'word': 'makes', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'm', 'prefix-2': 'ma', 'prefix-3': 'mak', 'suffix-1': 's', 'suffix-2': 'es', 'suffix-3': 'kes', 'prev_word': '*T*-2', 'next_word': 'judges', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'judges', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'j', 'prefix-2': 'ju', 'prefix-3': 'jud', 'suffix-1': 's', 'suffix-2': 'es', 'suffix-3': 'ges', 'prev_word': 'makes', 'next_word': 'sometimes', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'sometimes', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 's', 'prefix-2': 'so', 'prefix-3': 'som', 'suffix-1': 's', 'suffix-2': 'es', 'suffix-3': 'mes', 'prev_word': 'judges', 'next_word': 'wish', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'wish', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'w', 'prefix-2': 'wi', 'prefix-3': 'wis', 'suffix-1': 'h', 'suffix-2': 'sh', 'suffix-3': 'ish', 'prev_word': 'sometimes', 'next_word': '*-3', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': '*-3', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': True, 'prefix-1': '*', 'prefix-2': '*-', 'prefix-3': '*-3', 'suffix-1': '3', 'suffix-2': '-3', 'suffix-3': '*-3', 'prev_word': 'wish', 'next_word': 'to', 'has_hyphen': True, 'is_numeric': False, 'capitals_inside': False}, {'word': 'to', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 't', 'prefix-2': 'to', 'prefix-3': 'to', 'suffix-1': 'o', 'suffix-2': 'to', 'suffix-3': 'to', 'prev_word': '*-3', 'next_word': 'offer', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'offer', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'o', 'prefix-2': 'of', 'prefix-3': 'off', 'suffix-1': 'r', 'suffix-2': 'er', 'suffix-3': 'fer', 'prev_word': 'to', 'next_word': 'a', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'a', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'a', 'prefix-2': 'a', 'prefix-3': 'a', 'suffix-1': 'a', 'suffix-2': 'a', 'suffix-3': 'a', 'prev_word': 'offer', 'next_word': 'kind', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'kind', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'k', 'prefix-2': 'ki', 'prefix-3': 'kin', 'suffix-1': 'd', 'suffix-2': 'nd', 'suffix-3': 'ind', 'prev_word': 'a', 'next_word': 'of', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'of', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'o', 'prefix-2': 'of', 'prefix-3': 'of', 'suffix-1': 'f', 'suffix-2': 'of', 'suffix-3': 'of', 'prev_word': 'kind', 'next_word': 'Solomonic', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'Solomonic', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'prefix-1': 'S', 'prefix-2': 'So', 'prefix-3': 'Sol', 'suffix-1': 'c', 'suffix-2': 'ic', 'suffix-3': 'nic', 'prev_word': 'of', 'next_word': 'aid', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'aid', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'a', 'prefix-2': 'ai', 'prefix-3': 'aid', 'suffix-1': 'd', 'suffix-2': 'id', 'suffix-3': 'aid', 'prev_word': 'Solomonic', 'next_word': 'to', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'to', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 't', 'prefix-2': 'to', 'prefix-3': 'to', 'suffix-1': 'o', 'suffix-2': 'to', 'suffix-3': 'to', 'prev_word': 'aid', 'next_word': 'those', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'those', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 't', 'prefix-2': 'th', 'prefix-3': 'tho', 'suffix-1': 'e', 'suffix-2': 'se', 'suffix-3': 'ose', 'prev_word': 'to', 'next_word': 'who', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'who', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'w', 'prefix-2': 'wh', 'prefix-3': 'who', 'suffix-1': 'o', 'suffix-2': 'ho', 'suffix-3': 'who', 'prev_word': 'those', 'next_word': '*T*-4', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': '*T*-4', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': '*', 'prefix-2': '*T', 'prefix-3': '*T*', 'suffix-1': '4', 'suffix-2': '-4', 'suffix-3': '*-4', 'prev_word': 'who', 'next_word': \"'ve\", 'has_hyphen': True, 'is_numeric': False, 'capitals_inside': True}, {'word': \"'ve\", 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': \"'\", 'prefix-2': \"'v\", 'prefix-3': \"'ve\", 'suffix-1': 'e', 'suffix-2': 've', 'suffix-3': \"'ve\", 'prev_word': '*T*-4', 'next_word': 'been', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'been', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'b', 'prefix-2': 'be', 'prefix-3': 'bee', 'suffix-1': 'n', 'suffix-2': 'en', 'suffix-3': 'een', 'prev_word': \"'ve\", 'next_word': 'hurt', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'hurt', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'h', 'prefix-2': 'hu', 'prefix-3': 'hur', 'suffix-1': 't', 'suffix-2': 'rt', 'suffix-3': 'urt', 'prev_word': 'been', 'next_word': '*-1', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': '*-1', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': True, 'prefix-1': '*', 'prefix-2': '*-', 'prefix-3': '*-1', 'suffix-1': '1', 'suffix-2': '-1', 'suffix-3': '*-1', 'prev_word': 'hurt', 'next_word': '.', 'has_hyphen': True, 'is_numeric': False, 'capitals_inside': False}, {'word': '.', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': True, 'prefix-1': '.', 'prefix-2': '.', 'prefix-3': '.', 'suffix-1': '.', 'suffix-2': '.', 'suffix-3': '.', 'prev_word': '*-1', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}]\n",
            "['PRP', 'MD', 'VB', 'CC', 'VB', 'DT', 'NN', 'WDT', '-NONE-', 'VBZ', 'NNS', 'RB', 'VB', '-NONE-', 'TO', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NN', 'TO', 'DT', 'WP', '-NONE-', 'VBP', 'VBN', 'VBN', '-NONE-', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn_crfsuite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJmCOO1YAjH0",
        "outputId": "cee977e1-cc10-4345-fee5-e0e47ecdd58f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sklearn_crfsuite in /usr/local/lib/python3.8/dist-packages (0.3.6)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from sklearn_crfsuite) (0.8.10)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.8/dist-packages (from sklearn_crfsuite) (0.9.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sklearn_crfsuite) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.8/dist-packages (from sklearn_crfsuite) (4.64.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn_crfsuite import CRF\n",
        "\n",
        "model = CRF()\n",
        "\n",
        "try:\n",
        "    model.fit(X_train, y_train)\n",
        "except AttributeError:\n",
        "    pass\n",
        "predictions = model.predict(X_test)"
      ],
      "metadata": {
        "id": "eauDyjsDbQXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_of_model(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nto168TkCtsG",
        "outputId": "8ef98071-76d3-442e-c612-a3d954d7ee44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4024514811031665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sent = str(input())\n",
        "tokens = word_tokenize(input_sent)\n",
        "\n",
        "def pos_tag(sentence):\n",
        "    sentence_features = [features(sentence, index) for index in range(len(sentence))]\n",
        "    return list(zip(sentence, model.predict([sentence_features])[0]))\n",
        "\n",
        "print(pos_tag(tokens))  # [('I', 'PRP'), ('am', 'VBP'), ('Bob', 'NNP'), ('!', '.')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ehr1k5aAhyA",
        "outputId": "b1276398-0073-4b2e-f8c1-aa34e9cbc1c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am Tom\n",
            "[('I', 'PRP'), ('am', 'VBP'), ('Tom', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Linear Regression"
      ],
      "metadata": {
        "id": "kVz2qhitEA2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "vectorizer = DictVectorizer(sparse = True)\n",
        "lr = LogisticRegression(solver = 'lbfgs', multi_class = 'auto')\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', vectorizer),\n",
        "    ('classifier', lr)\n",
        "])\n",
        "\n",
        "pipeline.fit([features(sent, i) for sent in training_sentences for i in range(len(sent))],\n",
        "             [sent[i][1] for sent in training_sentences for i in range(len(sent))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PZ4tbeJY6vK",
        "outputId": "3aeb1b35-0162-48ef-c943-c4a6200ddd5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer', DictVectorizer()),\n",
              "                ('classifier', LogisticRegression())])"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = pipeline.predict([features(sent, i) for sent in test_sentences for i in range(len(sent))])"
      ],
      "metadata": {
        "id": "MEM8kxanaPSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "y_test_trans = list(chain.from_iterable(y_test))"
      ],
      "metadata": {
        "id": "WvPVVd2mfq8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "print(sklearn.metrics.accuracy_score(y_test_trans, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKn26P5hc2sa",
        "outputId": "0837bd0e-7936-4188-a482-4570955095f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sent = str(input())\n",
        "tokens = word_tokenize(input_sent)\n",
        "X = [features(tokens, i) for i in range(len(tokens))]\n",
        "print(tokens)\n",
        "print(pipeline.predict(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebHRaL62iBxY",
        "outputId": "207a3ced-b106-4737-f6a4-33d4e3ac95b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am very tired\n",
            "['I', 'am', 'very', 'tired']\n",
            "['PRP' 'DT' 'NN' 'NN']\n"
          ]
        }
      ]
    }
  ]
}