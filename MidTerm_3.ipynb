{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dW4bx-fH8xu1"},"outputs":[],"source":["import pandas as pd\n","import nltk\n","from nltk.util import ngrams\n","from collections import defaultdict, Counter\n","from nltk.tokenize import word_tokenize\n","import random\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OuJY0_d38xu4"},"outputs":[],"source":["csv_file = 'data.csv'\n","data = pd.read_csv(csv_file)\n","texts = data['content'].tolist()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wcNGe0sA8xu5"},"outputs":[],"source":["class generate:\n","    def __init__(self, n=2, gamma=0.1):\n","        self.n = n\n","        self.gamma = gamma\n","        all_ngrams = []\n","\n","        for text in texts:\n","            tokens = word_tokenize(text)\n","            ngram = list(ngrams(tokens, n, pad_left=True, pad_right=True))\n","            all_ngrams.extend(ngram)\n","        self.model = defaultdict(Counter)\n","\n","        for ngram in all_ngrams:\n","            context, word = tuple(ngram[:-1]), ngram[-1]\n","            self.model[context][word] += self.gamma\n","\n","        # Smoothing (Laplace)\n","        for context in self.model:\n","            total = float(sum(self.model[context].values()))\n","            for word in self.model[context]:\n","                self.model[context][word] = (self.model[context][word] + self.gamma) / (total + len(self.model[context]))\n","    def getModel(self):\n","        return self.model\n","    def generate_next_word(self, input_text, model):\n","        tokens = word_tokenize(input_text)\n","        context = tuple(tokens[-(self.n - 1):])\n","\n","        if context in model:\n","            next_word = random.choices(list(model[context].keys()), list(model[context].values()))[0]\n","            return next_word\n","        else:\n","            return None\n","\n","    def generate_text(self, input_text, max_len=30):\n","        model = self.getModel()\n","        tokens = word_tokenize(input_text)\n","        output = tokens.copy()\n","\n","        for _ in range(max_len):\n","            next_word = self.generate_next_word(' '.join(tokens[-(self.n - 1):]), model)\n","            if next_word is None:\n","                break\n","            output.append(next_word)\n","            # tokens.pop(0)\n","            tokens.append(next_word)\n","\n","        return ' '.join(output)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CC4OHiQ78xu6","outputId":"3a1a2122-6c43-4874-be17-a7ccc7378f75"},"outputs":[{"name":"stdout","output_type":"stream","text":["Đơn hàng chưa phục hồi , thế nhưng đã bị thế nào ?\n"]}],"source":["input_text = \"Đơn hàng chưa\"\n","g = generate(3, 0.01)\n","output_text = g.generate_text(input_text)\n","print(output_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y-swFapd8xu7","outputId":"e019e776-d1b4-46ce-ab5d-e42ebfac7e6a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Đơn hàng chưa niêm yết tại Sở cũng tích cực với trung tâm kiểm soát lạm phát lõi của các chiến hạm của hải quân Wonsan ở vùng gần tâm áp thấp nhiệt\n"]}],"source":["input_text = \"Đơn hàng chưa\"\n","g = generate(3, 0.1)\n","output_text = g.generate_text(input_text)\n","print(output_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zXGLXIFi8xu7","outputId":"cd65b82a-0c86-413a-ace7-a855831e740f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Đơn hàng chưa ? - 5 người Việt Nam cũng cho biết đang ăn phải ) , sói chi nhánh TPHCM ; Thông điệp ý thức hóa trong nước khác như oxy hóa\n"]}],"source":["input_text = \"Đơn hàng chưa\"\n","g = generate(2, 0.01)\n","output_text = g.generate_text(input_text)\n","print(output_text)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}